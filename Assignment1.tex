\documentclass{report}
% Change "article" to "report" to get rid of page number on title page
\usepackage{amsmath,amsfonts,amsthm,amssymb}
\usepackage{setspace}
\usepackage{Tabbing}
\usepackage{fancyhdr}
\usepackage{lastpage}
\usepackage{extramarks}
\usepackage{chngpage}
\usepackage{soul,color}
\usepackage{listings}
\usepackage{enumerate}
\usepackage{graphicx,float,wrapfig}
\usepackage{pifont}
\usepackage{graphicx}
\usepackage[english]{babel}
\usepackage{tikz}
\usepackage[]{algorithm2e}
% In case you need to adjust margins:
\topmargin=-0.45in      %
\evensidemargin=0in     %
\oddsidemargin=0in      %
\textwidth=6.5in        %
\textheight=9.0in       %
\headsep=0.25in         %

\title{Assignment 1 - Comp 632 - Machine Learning}

% Homework Specific Information
\newcommand{\hmwkTitle}{Assignment 1}                     % Adjust this
\newcommand{\hmwkDueDate}{Tuesday, January 27 2015}                           % Adjust this
\newcommand{\hmwkClass}{COMP 632}


\newcommand{\hmwkClassInstructor}{Dr. Doina Precup}
\newcommand{\hmwkAuthorName}{Geoffrey Stanley}
\newcommand{\hmwkAuthorNumber}{260645907}
\newcommand{\Pp}{\mathbb{P}}
\newcommand{\Ev}{\mathbb{E}}
\newcommand{\cov}{\text{Cov}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\dd}{\, \mathrm{d}}

% Setup the header and footer
\pagestyle{fancy}                                                       %
\lhead{\hmwkAuthorName}                              %
\chead{}
\rhead{\hmwkClass: \hmwkTitle}                                          %

\lfoot{}
\cfoot{}                                                                %
\rfoot{Page\ \thepage\ of\ \pageref{LastPage}}                          %
\renewcommand\headrulewidth{0.4pt}                                      %
\renewcommand\footrulewidth{0.4pt}                                      %

% This is used to trace down (pin point) problems
% in latexing a document:
%\tracingall
\definecolor{mygreen}{rgb}{0,0.6,0}
\lstset{commentstyle=\color{mygreen}, frame=single,  language=R, showspaces=false, showstringspaces=false}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Make title
\title{\vspace{2in}\textmd{\textbf{\hmwkClass:\ \hmwkTitle}}\\
\normalsize\vspace{0.1in}\small{Due\ on\ \hmwkDueDate}\\
\vspace{0.1in}\large{\textit{Presented to \hmwkClassInstructor}}\vspace{3in}}
\date{}
\author{\textbf{\hmwkAuthorName}\\
    \textbf{Student ID: \hmwkAuthorNumber}}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{document}
\maketitle
\section*{Question 1}
\subsection*{A)}
See source code.
\subsection*{B)}
Let $a$ be a vector equal to $[1/max_i |x_j|, ..., 1/max_i |x_n|]$. And let $A$
be a matrix such that $A=aI$. Substituting $X$ by $AX$ in the closed form regression
algorithm we obtain:
\begin{equation}
  \begin{aligned}
  2(AX)^TAXw &= 2(AX)^TY \\
  2A^TX^TAXw &= 2A^TX^TY \\
  X^TAXw &= X^TY \\
  Aw &= (X^TX)^{-1}X^TY \\
  \end{aligned}
\end{equation}
As such, we can conclude that normalization is equivalent to multiplying the $w$
vector by some scalar.
\subsection*{C)}
See source code.
\subsection*{D)}
See source code.
\subsection*{E)}
\begin{center}
    \begin{tabular}{| l | l | l | l | l | l | l | l |}
    \hline
    Fold & 1 & 2 & 3 & 4 & 5 & Avg & Std \\ \hline
    Training Error & 1.0728 & 0.7786 & 0.9651 & 0.7456 & 0.9245 & &  \\ \hline
    Testing Error & 0.5850 & 2.0271 & 0.9211 & 1.9323 & 0.9507 & & \\ \hline
    Best Hypothesis Class & 3 & 3 & 3 & 3 & 3 & & \\ \hline
    \end{tabular}
\end{center}
\begin{center}
    \begin{tabular}{| l | l |}
    \hline
    Fold & Weights \\ \hline
    1 & 54.8,    0.8,   -3.8,  120.5,
         -31.5,   -2.1,   -2.7 \\ \hline
    2 & 52.7,    5.9,   -1.5,  114.6,
         -32.1,   -1.2,   -2.4 \\ \hline
    3 & 51.1,   -1.0 ,   -1.4,  119.5,
         -31.0,   -1.1,   -2.7 \\ \hline
    4 & 52.9,   -5.0,   -3.8,  123.2,
         -30.3,   -1.2,   -2.9 \\ \hline
    5 & 54.2,   -5.8,   -3.0,  125.3,
         -31.3,   -1.6,   -3.2 \\ \hline
    \end{tabular}
\end{center}
\subsection*{F)}
\includegraphics[width=250pt, keepaspectratio=true]{error.png}
\includegraphics[width=250pt, keepaspectratio=true]{weights.png}
The graph on the left shows the impact of $\lambda$ on the mean squared error while
the graph on the right shows the impact of $\lambda$ on the weights associated to
the features being used.\\

These graphs show that as $\lambda$ increases it puts a greater pressure on the
features, decreasing their significance while increasing error. It also shows that
there are 5 features that are more predictive then the rest. We can conclude this
because the 4 features that quickly approach zero as $\lambda$ is increased.\\

Lastly we can see from the graph on the left that the optimum $\lambda$ will be
approximately between 5 and 6.
\section*{Question 2}
When simplifying the maximum likelihood equation for a regression whose variables
maintain a constant normal distribution we are able to obtain the sum-squared-error
function. However, when the standard deviation varies from one variable to another
this simplification is no longer achievable.
  \begin{equation}
     L(w) = \prod_{i=1}^{m} \frac{1}{\sqrt{2\pi\sigma_i^2}}e ^ {-\frac{1}{2}\left(\frac{y_i-h_w(x_i)}{\sigma_i}\right)^2}
  \end{equation}

  \begin{equation}
     \log L(w) = \sum_{i=1}^{m} \log \left(\frac{1}{\sqrt{2\pi\sigma_i^2}}\right)
     - \sum_{i=1}^{m} \frac{1}{2}\left(\frac{y_i-h_w(x_i)}{\sigma_i}\right)^2
  \end{equation}

  \begin{equation}
     \log L(w) = \sum_{i=1}^{m} \log \left((2\pi)^{-1/2}\sigma_i^{-1}\right)
     - \sum_{i=1}^{m} \frac{1}{2}\left(\frac{y_i-h_w(x_i)}{\sigma_i}\right)^2
  \end{equation}

  \begin{equation}
     \log L(w) = -\frac{1}{2} \log(2\pi)
     - \sum_{i=1}^{m} \log \sigma_i
     - \sum_{i=1}^{m} \frac{1}{2}\left(\frac{y_i-h_w(x_i)}{\sigma_i}\right)^2
  \end{equation}

\begin{equation}
     \frac{\partial}{\partial w_j} \log L(w)
     = \frac{\partial}{\partial w_j} \left(-\frac{1}{2} \log(2\pi)
     - \sum_{i=1}^{m} \log \sigma_i
     - \sum_{i=1}^{m} \frac{1}{2}\left(\frac{y_i-h_w(x_i)}{\sigma_i}\right)^2\right)
\end{equation}

\begin{equation}
     \frac{\partial}{\partial w_j} \log L(w) =
     - \sum_{i=1}^{m} \frac{x_i}{\sigma_i}\left(\frac{y_i-h_w(x_i)}{\sigma_i}\right)
\end{equation}

Simplying further while converting standard deviation to variance. Where $\sigma^2 = \Omega$.

\begin{equation}
     \frac{\partial}{\partial w_j} \log L(w) =
     \sum_{i=1}^{m} \frac{x_iy_i}{\Omega_i}
     - \sum_{i=1}^{m}\frac{h_w(x_i)x_i}{\Omega_i}
\end{equation}

\begin{equation}
     \frac{\partial}{\partial w_j} \log L(w) =
     X^T\Omega^{-1}Y - wX^T\Omega^{-1}X
\end{equation}

Now setting the gradient to zero.

\begin{equation}
     0 =X^T\Omega^{-1}Y
     - wX^T\Omega^{-1}X
\end{equation}

\begin{equation}
     w^* = (X^T\Omega^{-1} X)^{-1} X^T\Omega^{-1} Y
\end{equation}


\section*{Question 3}
Given the following huberized loss function:
\begin{equation}
  L_H(w, \delta) = \left\{
  \begin{array}{l l}
    (y_i - w^Tx_i)^2 / 2 & \quad \text{if }  | y_i - w^Tx_i | \leq \delta\\
    \delta | y_i - w^Tx_i | - \delta^2 / 2 & \quad \text{otherwise}

  \end{array} \right.\end{equation}


\subsection*{A)}
Find the derivative:

\begin{equation}
  \frac{\partial}{\partial w} L_H(w, \delta) = \left\{
  \begin{array}{l l}
    \frac{\partial}{\partial w} \left((y_i - w^Tx_i)^2 / 2 \right)& \quad \text{if }  | y_i - w^Tx_i | \leq \delta\\
    \frac{\partial}{\partial w} \left(\delta | y_i - w^Tx_i | - \delta^2 / 2\right) & \quad \text{otherwise}

  \end{array} \right.\end{equation}
  \\

The equation $| y_i - w^Tx_i |$ is equivalent to $\sqrt{( y_i - w^Tx_i )^2}$ as such
the derivative becomes:

  \begin{equation}
    \frac{\partial}{\partial w} L_H(w, \delta) = \left\{
    \begin{array}{l l}
      - x_i (y_i - w^Tx_i)  & \quad \text{if }  | y_i - w^Tx_i | \leq \delta\\
      - x_i \delta \frac{y_i - w^Tx_i}{| y_i - w^Tx_i |}  & \quad \text{otherwise}

    \end{array} \right.\end{equation}
    \\

The expression $\frac{y_i - w^Tx_i}{| y_i - w^Tx_i |}$ is equivalent to the sign
of $y_i - w^Tx_i$ as such the derivative can be written as:

\begin{equation}
  \frac{\partial}{\partial w} L_H(w, \delta) = \left\{
  \begin{array}{l l}
    - x_i (y_i - w^Tx_i) & \quad \text{if }  | y_i - w^Tx_i | \leq \delta\\
    - x_i \delta sign ( y_i - w^Tx_i ) & \quad \text{otherwise}

  \end{array} \right.\end{equation}
  \\

Where $sign(y_i - w^Tx_i)$ is replaced by 1 or -1 depending on whether the result of the
equation is positive or negative.

\subsection*{B)}
The algorithm was implemented in python source code and is defined as function
GradientDescent.
\\
\begin{algorithm}[H]
 \KwData{a dataset containing features and targets, learning rate, lambda, delta}
 \KwResult{a weight vector }

 initialize weights to 1 or random numbers\;

 \While{max iterations not achieved}{
   initialize gradient = 0\;
  \While{not at the last row of the data array}{

    hypothesis = dot product (weights , row of features)\;
    regularizor = lambda/2 * dot product(transpose of weights, weights)\;
    \eIf{$| hypothesis | \leq delta$}{

      loss = hypothesis - row of targets\;
      J = dot product ( transpose of the row features, loss )\;
      gradient += J + regularizor\;

     }{
     loss = hypothesis - row of targets\;
     \eIf{loss is negative}{
       gradient += delta * row of features + regularizor\;
     }{
       gradient -= delta * row of features + regularizor\;
     }
    }

    go to the next row\;
  }
  \If{gradient * learning rate $\leq$ desired precision}{
    return weights\;
  }

  weights = weights + (learning rate) * gradient
 }
 return weights\;
 \caption{Gradient descent using huber loss function}
\end{algorithm}
\subsection*{C)}


\section*{Question 4}

  \begin{equation}
    h_{w1,...wk}(x)
    = \prod_{k=1}^{K}\phi_k(x)
  \end{equation}
Where:
  \begin{equation}
    \phi_k(x) = e^{w_k^Tx}
    = e^{\sum_{i=1}^m w_{ik}x_i}
  \end{equation}
Replacing $\phi_k(x)$

  \begin{equation}
    h_{w1,...wk}(x)
    = \prod_{k=1}^{K}e^{\sum_{i=1}^m w_{ik}x_i}
  \end{equation}

  \begin{equation}
    \log h_{w1,...wk}(x)
    = \sum_{k=1}^{K} \sum_{i=1}^m w_{ik}x_i
  \end{equation}

  \begin{equation}
    \frac{\partial}{\partial w} \log h_{w1,...wk}(x)
    = \frac{\partial}{\partial w} \left(\sum_{k=1}^{K} \sum_{i=1}^m w_{ik}x_i\right)
  \end{equation}

  \begin{equation}
    \frac{\partial}{\partial w} \log h_{w1,...wk}(x)
    =  \sum_{i=1}^m x_i
  \end{equation}

\end{document}
